{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Internship Assignment | Hypothizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1. How do you approach the problem of finding similarity between any two clusters of text? Please explain with logic?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the documents with similar context tends to have similar words.\n",
    "For the task of finding similarity between any two text clusters, first we have to convert the text into a more suitable representation like vectors (if not already converted) so that we can apply mathematical operations or measurements to our text documents/clusters. And then apply some metric too measure the similarity or distance between the vectors.\n",
    "\n",
    "Here I describe my approach to the problem of finding similarity b/w any 2 clusters of text in the following 5 steps.\n",
    "\n",
    "**Step 1: Preprocessing** - The unprocessed or raw text contains many unwanted structures or elements which provides no extra information about our documents and make our task more cumbersome and noisy.\n",
    "    \n",
    "   >**1.1 Remove stop words** - Stop words like a, the, is, are are the most frequent words in any documents but they do not contribute to any deeper meaning of the phrase. So, it is better if we remove these words.\n",
    "\n",
    "    >**1.2 Stemming** - It was found that various morphological variations of words often have a similar meaning. So, we should reduce these types of words to their root word. Example - \"stems\", \"stemmer\", \"stemming\", \"stemmed\" reduced to root \"stem\".\n",
    "\n",
    "    >**1.3 Removing infrequent words** - Infrequent words contribute a little to the similarity of the documents an produces noise in our data. So, it is better if we set a threshold and if a word appears less then that threshold time then we remove it.\n",
    "\n",
    "    >**1.4 Convert the documents to the same length** - Here we normalize our documents to the same length (often by adding paddings) so that large text documents in the cluster could not dominate the smaller one.\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "** Step 2: Bag-Of-Word **- Once we are done with preprocessing, we apply the method like Bag-Of-Word to the text documents. BOW model converts each text document into a vector representation where each word in the vocabulary represents a unique dimension of that vector.\n",
    "\n",
    ">**Example:**\n",
    ">For vocabulary - it, was, the, best, of, times, worst, age, wisdom, foolishness\n",
    "\n",
    ">The vector for document \"it was the worst of times\" is - [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
    "\n",
    "The logic behind is that the machine learning algorithms cannot work with raw text data and it needs to be converted to a more suitable representation of numbers. We can combine BOW with a scoring method like TF-IDF (Term Frequency- Inverse Document Frequency) in which the frequent words that are also frequent across all documents are penalized (assigned low weight). In this way, the common words which are non-descriptive for the topic of a document can be ignored.\n",
    "\n",
    "The output from this step is weighted term vector.\n",
    "\n",
    "**(Here, instead of BOW model we can also use Word embedding)**\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "**Step 3: Selecting a measure** -Now we have to define a metric to measure the similarity or distance between the vector representation of our documents.\n",
    "\n",
    "There are several measures available for that and the effectiveness of these measures are completely dependent on the task in our hand and the algorithm used to cluster the objects.\n",
    "\n",
    "Few of the measures are:\n",
    "\n",
    ">**3.1 Cosine Similarity** - As the documents are being changed into vectors the similarity between those vectors resembles the correlation between them.\n",
    "    The correlation between 2 vectors can be found mathematically by taking the cosine of the angle between them. The result is bounded between [0, 1] and it is 1 when the 2 vectors are identical.\n",
    "\n",
    "    >**3.2 Euclidean Distance** - Euclidean distance is simply the distance between two points and the concept can be extended to two or more dimensions. The less is the distance between two vectors the more similar the vectors are.\n",
    "\n",
    "    >**3.3 Pearson Correlation Coefficient** - Pearson Correlation Coefficient is another measure for the similarity between 2 vectors. The result ranges from -1 to +1, 1 when 2 documents are identical and -1 when there is no similarity between them.\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "**Step 4: Selecting a representative Vector** - Now we have to select a representative vector for our cluster. For example in clustering algorithm like K-means, the cluster is represented by the centroid object. \n",
    "\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "**Step 5: Measuring the similarity** - Finally we measure the similarity/distance between the vectors of the clusters using our chosen metric.\n",
    "\n",
    "(We can further use this approach to find Within cluster similarity/distance.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "###  P2. How do you approach sentence level correction? using language modeling or any other methods? Please explain with logic?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can approach the problem of sentence-level correction in 2 ways:**\n",
    "1. Using language modeling technique.\n",
    "2. Using Encoder-Decoder neural network with Attention.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**1. Using language modeling technique**\n",
    "\n",
    "\n",
    "The problem with natural languages is that there is no formal specification. There are rules for parts of the language but they are used very rarely. There are a large number of terms in NL that can be used in many different ways. \n",
    "\n",
    "Language modeling is the development of a probabilistic model which can give us the probability of occurrence of a word given the preceding words. \n",
    "\n",
    "Neural networks can be used in the development of the language models as Neural network models have more ability to generalize than the linear models and they allow conditioning on the large context sizes and then the language modeling is called neural network language modeling.\n",
    "\n",
    "\n",
    "\n",
    "**Step 1. Data collection** -  First we have to collect some error-free data. It is good for our model if we collect similar data for which we want to build the language model.\n",
    "\n",
    "**Step 2. Cleaning** - We have to clean the collected data by removing unwanted punctuations and other elements to minimize the size of our vocabulary.\n",
    "\n",
    "**Step 3. Modelling the data** - For the task of language modeling, we want our model to predict the probability of occurrence of a word given preceding words and then we fed that word again as the input to get next word. For that, we have to make our data suitable for this task by slicing it down into input sequences of some predefined length and unit length output (if we want to predict one word).\n",
    "\n",
    ">**Example** - We can covert long text into sequences of 20 input words and 1 output word.\n",
    "\n",
    "**Step 4. Tokenize the documents** - Train the tokenizer on the entire dataset which finds all the unique words in the dataset and assign them to the unique integers. These integers can be considered as the index of words and we can easily map those integers to their corresponding words later.\n",
    "\n",
    "**Step 5. Input and output sequences** - Now we convert text sequence into X(input) and Y(output) elements.\n",
    "Input sequence can be created easily with slicing, but we have to one-hot-code the output Y such that the size of output vector is equal to the size of vocabulary with all \"0\" except one \"1\" whose index maps the next word of the input sequence.\n",
    "\n",
    "**Step 6. Define and fit the model** - In this step, we define the model and fit it to the error-free data. In this way our model learn to represent the patterns in the text.\n",
    "\n",
    "Following layers with the combination of several other layers can be used:\n",
    "\n",
    "Embedding Layer - As text is not a suitable format to pass as input to an ML model, we create word embedding distributed vector representation of our text.\n",
    " \n",
    "Long short-term memory - These layer contains memory units and allow the model to learn the relevant context over much longer input sequences. More memory cells and a deeper network may achieve better results.\n",
    "\n",
    "Dense layer  - One or more dense layer should be used to interpret the features extracted from the sequence. And the last layer must be a dense layer with the size equals to the size of vocabulary and \"softmax\" as the activation function. The reason behind using softmax activation is that it gives the probability of the classes as output.\n",
    "\n",
    "**Step 7.** Finally, we fed the erroneous data to our model and check that if the next word resembles the suggestion of our model. If not, we can replace the erroneous word with the word suggested by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**2. Using Encoder-Decoder neural network with Attention**\n",
    "\n",
    "Encoder-Decoder models found its application in various sequence-to-sequence prediction tasks like machine translation. Our problem of sentence-level correction can be modeled as a problem for translating the erroneous sentences to the correct once and then various machine translation methods can be applied to the problem.\n",
    "\n",
    "It is really helpful for our problem if we apply the advantages of attention mechanism to improve the performance of the encoder-decoder model for long sequences. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "## P3. Coding Problem : (please explain your solution and write code in any language preferably - c/c++/python ) Longest Path in a Directed Acyclic Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.A.G "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As name suggest, a directed ayclic graph is a graph with no cycles. For this problem we assume that a graph is already a DAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Here we start by ceating a Node first\n",
    "Fields includes a value and the information about edges\n",
    "A flag Visited which tell if we visited the node during the search or not'''\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.edges = []\n",
    "        self.visited = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Class for creating edges\n",
    "The object contains a value and the informaion of the incoming \n",
    "and the outgoing edge from the node'''\n",
    "\n",
    "class Edge(object):\n",
    "    def __init__(self, value, node_from, node_to):\n",
    "        self.value = value\n",
    "        self.node_from = node_from\n",
    "        self.node_to = node_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Graph class with constructor initialize \n",
    "    with the informationof nodes and edges \n",
    "    (list of nodes and edges)'''\n",
    "class Graph(object):\n",
    "    def __init__(self, nodes=[], edges=[]):\n",
    "        self.nodes = nodes\n",
    "        self.edges = edges\n",
    "\n",
    "        \n",
    "    #Function to inster the new node in the graph\n",
    "    def insert_node(self, new_node_val):\n",
    "        new_node = Node(new_node_val)\n",
    "        self.nodes.append(new_node)\n",
    "    \n",
    "    \n",
    "    #Function to insert a new edge to the graph\n",
    "    def insert_edge(self, new_edge_val, node_from_val, node_to_val):\n",
    "        from_found = None\n",
    "        to_found = None\n",
    "        for node in self.nodes:\n",
    "            if node_from_val == node.value:\n",
    "                from_found = node\n",
    "            if node_to_val == node.value:\n",
    "                to_found = node\n",
    "        if from_found == None:\n",
    "            from_found = Node(node_from_val)\n",
    "            self.nodes.append(from_found)\n",
    "        if to_found == None:\n",
    "            to_found = Node(node_to_val)\n",
    "            self.nodes.append(to_found)\n",
    "        new_edge = Edge(new_edge_val, from_found, to_found)\n",
    "        from_found.edges.append(new_edge)\n",
    "        to_found.edges.append(new_edge)\n",
    "        self.edges.append(new_edge)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ''' Function to return the list of edges in the graph\n",
    "        Return a list of triples like this:\n",
    "        From node value, to node value'''\n",
    "    def get_edge_list(self):\n",
    "        edge_list = []\n",
    "        for edge_object in self.edges:\n",
    "            edge = (edge_object.node_from.value, edge_object.node_to.value)\n",
    "            edge_list.append(edge)\n",
    "        return edge_list\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # finding the max index\n",
    "    def find_max_index(self):\n",
    "        max_index = -1\n",
    "        if len(self.nodes):\n",
    "            for node in self.nodes:\n",
    "                if node.value > max_index:\n",
    "                    max_index = node.value\n",
    "        return max_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "### After Creating the DAG we define a function for Depth First Search which will return a list of nodes it encounter while performing the search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth First Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function return a path for Depth first search\n",
    "def Depth_first_search(graph,current,visited,cpath):\n",
    "    npath = [] #new path\n",
    "    for node in graph[current]:\n",
    "        if node not in visited:\n",
    "            visited.append(node)\n",
    "            tpath = cpath + [node] #tmp path\n",
    "            npath.append(tpath)    #append to existing path\n",
    "            #extend current path\n",
    "            npath.extend(Depth_first_search(graph,node,visited,tpath))\n",
    "    return npath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we define a function which takes source and graph name as input and return all the paths + the longest path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "## Findfing Longest path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def longest_path(source,graph_name):\n",
    "    edges_list  = graph_name.get_edge_list()\n",
    "\n",
    "    #Converting edges in an directed graph to adjacency list\n",
    "    graph = defaultdict(list) #dictionary of lists\n",
    "    for (s,t) in edges_list:\n",
    "        graph[s].append(t)\n",
    "        \n",
    "        \n",
    "    visited = [1]\n",
    "    path = [1]\n",
    "    result  = Depth_first_search(graph,source,visited,path)\n",
    "        \n",
    "    print\"Paths from the source node:\",result\n",
    "    result.sort(key=len,reverse=True)\n",
    "    print(\"Longest path:\", result[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "## Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
